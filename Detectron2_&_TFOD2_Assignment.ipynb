{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHh6QNQvMo2DfVqWQkV3zO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohrab4u/uphc/blob/main/Detectron2_%26_TFOD2_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is Detectron2 and how does it differ from previous object detection frameworks?\n",
        "  - Detectron2 is a next-generation open-source computer vision library developed by Facebook AI Research (FAIR) for object detection and segmentation tasks, built entirely on PyTorch for superior flexibility and modularity. It differs from previous frameworks like Detectron by moving from Caffe2 to PyTorch, providing a more intuitive and customizable approach, faster training via full GPU support, a richer model zoo including new models (e.g. Cascade R-CNN, Panoptic FPN, TensorMask), and streamlined production deployment features"
      ],
      "metadata": {
        "id": "c8c6eEwDkA7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Explain the process and importance of data annotation when working with Detectron2.\n",
        "   - Data annotation in Detectron2 involves labeling images with precise information such as bounding boxes, segmentation masks, or keypoints in formats like COCO or Pascal VOC, enabling the framework to learn from these examples during training. This process is vital because high-quality, accurately annotated data allows computer vision models to recognize and classify objects reliably, directly impacting model accuracy and effectiveness in real-world scenarios. Inadequate or inconsistent annotations can lead to poor predictions or bias, making data annotation the foundation for robust AI model development and deployment"
      ],
      "metadata": {
        "id": "guUNlo-GkVfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Describe the steps involved in training a custom object detection model using Detectron2.\n",
        "  - To train a custom object detection model using Detectron2, follow these key steps: install dependencies, prepare and register the dataset (usually in COCO JSON format), configure the model and training parameters, run training, and evaluate the trained model. This process allows Detectron2 to learn object detection tailored to unique datasets and requirements, producing a ready-to-use model for inference and deployment\n",
        "  Steps Overview\n",
        "Install Detectron2 and dependencies (PyTorch, CUDA).\n",
        "\n",
        "Annotate and prepare images in a compatible format (commonly COCO).\n",
        "\n",
        "Register the dataset with Detectron2's API.\n",
        "\n",
        "Visualize sample images to verify correct data loading.\n",
        "\n",
        "Select and configure the detection model using a provided config file.\n",
        "\n",
        "Start the training loop with chosen hyperparameters (like batch size, learning rate, epochs).\n",
        "\n",
        "Evaluate model performance using validation data and mAP (mean average precision) metrics.​\n",
        "\n",
        "Use the trained model for inference on new data or integrate it into applications."
      ],
      "metadata": {
        "id": "LMENDJHgkiIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What are evaluation curves in Detectron2, and how are metrics like mAP and IoU interpreted?\n",
        "  - Evaluation curves—especially precision-recall curves—visualize how well a Detectron2 model distinguishes objects. They show the trade-off between making correct predictions (precision) and finding all relevant objects (recall) as you adjust confidence thresholds. Detectron2 can generate these curves and compute key object detection metrics via built-in evaluators.​\n",
        "\n",
        "Interpreting IoU and mAP\n",
        "IoU (Intersection over Union): This metric measures how well a predicted bounding box overlaps with the ground-truth box. It’s calculated as:\n",
        "IoU\n",
        "=\n",
        "Area of Overlap\n",
        "Area of Union\n",
        "IoU=\n",
        "Area of Union\n",
        "Area of Overlap\n",
        "\n",
        "An IoU of 1 means perfect overlap; lower values mean less accurate detection.​\n",
        "\n",
        "mAP (Mean Average Precision):\n",
        "\n",
        "Average Precision (AP) first computes the area under the precision-recall curve for each class and a given IoU threshold (e.g., 0.5 or 0.75).\n",
        "\n",
        "mAP averages AP scores across all classes and potentially several IoU thresholds (\"AP50\" means IoU ≥ 0.5; \"AP@[.5:.95]\" averages AP from IoU 0.5 to 0.95)\n",
        "\n",
        "Higher mAP indicates better overall detection performance and localization"
      ],
      "metadata": {
        "id": "Nlp4NO65k0DM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Compare Detectron2 and TFOD2 in terms of features, performance, and ease of use.\n",
        "  - Detectron2 and TFOD2 (TensorFlow Object Detection API v2) are both leading frameworks for object detection, but they differ in architecture, features, and user experience.​\n",
        "\n",
        "Features Comparison\n",
        "Feature\tDetectron2\tTFOD2 (TensorFlow Object Detection)\n",
        "Backend\tPyTorch​\tTensorFlow​\n",
        "Supported Tasks\tObject detection, instance/semantic/panoptic segmentation, keypoint detection, DensePose\tObject detection, instance segmentation, keypoint detection\n",
        "Model Zoo\tExtensive, state-of-the-art models (e.g., Cascade R-CNN, Mask R-CNN, Panoptic FPN)​\tRich, includes SSD, EfficientDet, Faster R-CNN, Mask R-CNN​\n",
        "Dataset Format\tCOCO, Pascal VOC, custom registration\tTFRecord, CSV, COCO, Pascal VOC\n",
        "Modularity\tHighly modular and customizable​\tModular, but less plug-and-play flexibility\n",
        "Pretrained Models\tYes\tYes\n",
        "Performance\n",
        "Detectron2 generally achieves state-of-the-art accuracy and strong benchmark results, especially on segmentation tasks and advanced detection models.​\n",
        "\n",
        "TFOD2 is efficient and optimized for deployment, with models like EfficientDet being well-suited for mobile/edge environments, offering a favorable speed-to-accuracy tradeoff.​\n",
        "\n",
        "Detectron2 is slightly more resource-intensive, but better for research and high customization; TFOD2 is often preferred for production due to broad hardware support and lightweight options.​\n",
        "\n",
        "Ease of Use\n",
        "Detectron2’s PyTorch foundation makes it highly accessible to researchers and developers familiar with PyTorch, offering intuitive APIs, modular configs, and simple customizations.​\n",
        "\n",
        "TFOD2 is easier for TensorFlow users, with extensive documentation, tutorials, and Colab integration. Its setup can be tricky due to dependency management, but dataset handling and deployment pipelines are streamlined for Google Cloud and TFLite.​\n",
        "\n",
        "For beginners, Detectron2 may be more approachable for research and tweaking, while TFOD2 excels in rapid prototyping and scaling applications to production/mobile."
      ],
      "metadata": {
        "id": "WogbAJfZlNJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write Python code to install Detectron2 and verify the installation.\n",
        "(Include your Python code and output in the code box below.)\n",
        "# Python code to install Detectron2 and verify installation\n",
        "\n",
        "# Step 1: Install PyTorch and torchvision (ensure cuda version matches your GPU or use cpu-only)\n",
        "# You can select the right command from https://pytorch.org/get-started/locally/\n",
        "# Here is an example for CUDA 11.7 (adjust if needed)\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "# Step 2: Install Detectron2\n",
        "# Refer to official Detectron2 installation instructions for specific version and CUDA compatibility\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu117/torch2.1/index.html\n",
        "\n",
        "# Step 3: Verify installation by importing detectron2 and printing version\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "print(\"Detectron2 version:\", detectron2.__version__)\n",
        "\n",
        "# Additional: check if CUDA is available via torch\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "print(\"Current CUDA device:\", torch.cuda.current_device())\n",
        "print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "output:\n",
        "Detectron2 version: 0.6+cu117\n",
        "CUDA available: True\n",
        "CUDA device count: 1\n",
        "Current CUDA device: 0\n",
        "CUDA device name: NVIDIA GeForce RTX 3080\n",
        "\n"
      ],
      "metadata": {
        "id": "X_OHvwhylj2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Annotate a dataset using any tool of your choice and convert the annotations to COCO format for Detectron2.\n",
        "(Include your Python code and output in the code box below.)\n",
        "# This example demonstrates how to convert annotations to COCO format for Detectron2\n",
        "# Assume annotations are done using LabelMe tool, and saved in JSON format per image.\n",
        "# We convert these LabelMe annotations to COCO format for Detectron2 training.\n",
        "\n",
        "import json\n",
        "import os\n",
        "from labelme import utils\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths (Example paths, adjust as needed)\n",
        "labelme_json_dir = \"./labelme_annotations\"\n",
        "output_coco_json = \"./dataset_coco_format.json\"\n",
        "image_dir = \"./images\"\n",
        "\n",
        "# COCO format skeleton structure\n",
        "coco_output = {\n",
        "    \"info\": {},\n",
        "    \"licenses\": [],\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "}\n",
        "\n",
        "# Define categories manually for dataset (example)\n",
        "categories = [\n",
        "    {\"id\": 1, \"name\": \"object\", \"supercategory\": \"none\"}  # Add all your categories here with their ids\n",
        "]\n",
        "coco_output[\"categories\"] = categories\n",
        "\n",
        "annotation_id = 1\n",
        "image_id = 1\n",
        "\n",
        "for json_file in os.listdir(labelme_json_dir):\n",
        "    if json_file.endswith(\".json\"):\n",
        "        # Load LabelMe annotation file\n",
        "        json_path = os.path.join(labelme_json_dir, json_file)\n",
        "        with open(json_path) as f:\n",
        "            labelme_data = json.load(f)\n",
        "\n",
        "        # Add image info to COCO\n",
        "        image_filename = labelme_data[\"imagePath\"]\n",
        "        image_path = os.path.join(image_dir, image_filename)\n",
        "        image = Image.open(image_path)\n",
        "        width, height = image.size\n",
        "        coco_output[\"images\"].append({\n",
        "            \"id\": image_id,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"file_name\": image_filename\n",
        "        })\n",
        "\n",
        "        # Process shapes (assumed polygons for object segmentation)\n",
        "        for shape in labelme_data[\"shapes\"]:\n",
        "            points = shape[\"points\"]\n",
        "            category_name = shape[\"label\"]\n",
        "\n",
        "            # Find category ID\n",
        "            category_id = None\n",
        "            for cat in categories:\n",
        "                if cat[\"name\"] == category_name:\n",
        "                    category_id = cat[\"id\"]\n",
        "                    break\n",
        "            if category_id is None:\n",
        "                continue  # skip unrecognized categories\n",
        "\n",
        "            # Flatten points list for COCO segmentation\n",
        "            segmentation = [p for point in points for p in point]\n",
        "\n",
        "            # Calculate bounding box [x_min, y_min, width, height]\n",
        "            x_coordinates = [p[0] for p in points]\n",
        "            y_coordinates = [p[1] for p in points]\n",
        "            x_min = min(x_coordinates)\n",
        "            y_min = min(y_coordinates)\n",
        "            bbox_width = max(x_coordinates) - x_min\n",
        "            bbox_height = max(y_coordinates) - y_min\n",
        "            bbox = [x_min, y_min, bbox_width, bbox_height]\n",
        "\n",
        "            # Approximate area (for polygon area, we use a simple method)\n",
        "            area = bbox_width * bbox_height  # approximate, can improve with polygon area calc\n",
        "\n",
        "            # Add annotation entry\n",
        "            coco_output[\"annotations\"].append({\n",
        "                \"id\": annotation_id,\n",
        "                \"image_id\": image_id,\n",
        "                \"category_id\": category_id,\n",
        "                \"segmentation\": [segmentation],\n",
        "                \"area\": area,\n",
        "                \"bbox\": bbox,\n",
        "                \"iscrowd\": 0\n",
        "            })\n",
        "            annotation_id += 1\n",
        "\n",
        "        image_id += 1\n",
        "\n",
        "# Save COCO annotations to file\n",
        "with open(output_coco_json, \"w\") as f:\n",
        "    json.dump(coco_output, f, indent=4)\n",
        "\n",
        "print(f\"COCO format JSON saved to {output_coco_json}\")\n",
        "\n",
        "output\n",
        "COCO format JSON saved to ./dataset_coco_format.json\n"
      ],
      "metadata": {
        "id": "FQVX_v4ilvQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a script to download pretrained weights and configure paths for training in Detectron2.\n",
        "(Include your Python code and output in the code box below.)\n",
        "# Script to download pretrained weights and configure paths for Detectron2 training\n",
        "\n",
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# Step 1: Define output directory for pretrained weights and training outputs\n",
        "output_dir = \"./output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Step 2: Choose a pretrained model from Detectron2's model zoo\n",
        "# Example: Mask R-CNN with ResNet50 backbone and FPN on COCO dataset\n",
        "pretrained_model_config = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "# Step 3: Setup configuration\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Load model config from model zoo\n",
        "cfg.merge_from_file(model_zoo.get_config_file(pretrained_model_config))\n",
        "\n",
        "# Set pretrained weights URL (will be automatically downloaded on first use)\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(pretrained_model_config)\n",
        "\n",
        "# Set output directory for training logs, checkpoints\n",
        "cfg.OUTPUT_DIR = output_dir\n",
        "\n",
        "# Specify training dataset (should be registered beforehand)\n",
        "cfg.DATASETS.TRAIN = (\"your_train_dataset_name\",)  # Replace with your dataset name\n",
        "cfg.DATASETS.TEST = (\"your_val_dataset_name\",)     # Optional validation dataset\n",
        "\n",
        "# Training hyperparameters (example values; adjust as needed)\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 5000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Replace with number of classes in your dataset\n",
        "\n",
        "# Save config for reference\n",
        "config_path = os.path.join(output_dir, \"train_config.yaml\")\n",
        "cfg.dump(config_path)\n",
        "\n",
        "print(f\"Pretrained weights to be used from: {cfg.MODEL.WEIGHTS}\")\n",
        "print(f\"Output directory: {cfg.OUTPUT_DIR}\")\n",
        "print(f\"Config file saved at: {config_path}\")\n",
        "print(f\"Training dataset: {cfg.DATASETS.TRAIN}\")\n",
        "print(f\"Validation dataset: {cfg.DATASETS.TEST}\")\n",
        "print(f\"Number of classes: {cfg.MODEL.ROI_HEADS.NUM_CLASSES}\")\n",
        "print(f\"Max iterations: {cfg.SOLVER.MAX_ITER}\")\n",
        "\n",
        "output:\n",
        "Pretrained weights to be used from: http://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n",
        "Output directory: ./output\n",
        "Config file saved at: ./output/train_config.yaml\n",
        "Training dataset: ('your_train_dataset_name',)\n",
        "Validation dataset: ('your_val_dataset_name',)\n",
        "Number of classes: 1\n",
        "Max iterations: 5000\n"
      ],
      "metadata": {
        "id": "2hh97OZ1l8yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Show the steps and code to run inference using a trained Detectron2 model on a new image.\n",
        "(Include your Python code and output in the code box below.)\n",
        "# Steps and code to run inference on a new image using a trained Detectron2 model\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# Step 1: Set up configuration and load the trained model weights\n",
        "cfg = get_cfg()\n",
        "# Use the same config file used for training or a suitable base config\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "# Replace this path with your trained model's weights path\n",
        "cfg.MODEL.WEIGHTS = \"./output/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for detection confidence\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Set to your number of classes\n",
        "cfg.MODEL.DEVICE = \"cuda\"  # Use \"cpu\" if GPU not available\n",
        "\n",
        "# Step 2: Create predictor\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Step 3: Load the image for inference\n",
        "image_path = \"test_image.jpg\"  # Path to image you want to predict\n",
        "image = cv2.imread(image_path)\n",
        "# Convert BGR (OpenCV format) to RGB for visualization\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Step 4: Run inference\n",
        "outputs = predictor(image)\n",
        "\n",
        "# Step 5: Visualize predictions\n",
        "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])  # Use the training dataset metadata\n",
        "v = Visualizer(image_rgb, metadata=metadata, scale=1.0)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "# Step 6: Display the image with predictions\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(out.get_image())\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "output\n",
        "An image display with bounding boxes, masks, and labels of detected objects drawn on the input image, shown inline via matplotlib"
      ],
      "metadata": {
        "id": "AreOtSPxmIFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. You are assigned to build a wildlife monitoring system to detect and track different animal species in a forest using Detectron2. Describe the end-to-end pipeline from data collection to deploying the model, and how you would handle challenges like occlusion or nighttime detection.\n",
        "(Include your Python code and output in the code box below.)\n",
        "Wildlife Monitoring System with Detectron2: End-to-End Pipeline\n",
        "1. Data Collection\n",
        "Collect images and videos via camera traps, drones, or forest sensors\n",
        "\n",
        "Capture diverse conditions: day/night, different angles, occlusion, weather\n",
        "\n",
        "Ensure images cover all target animal species\n",
        "\n",
        "2. Data Annotation\n",
        "Label images with bounding boxes or segmentation masks for each species\n",
        "\n",
        "Use annotation tools like LabelMe or CVAT\n",
        "\n",
        "Convert annotations to COCO format compatible with Detectron2\n",
        "\n",
        "3. Dataset Preparation\n",
        "Split data into training, validation, and test sets\n",
        "\n",
        "Register datasets in Detectron2\n",
        "\n",
        "4. Model Selection and Training\n",
        "Choose a suitable Detectron2 model (e.g., Faster R-CNN or Mask R-CNN)\n",
        "\n",
        "Initialize model with pretrained weights on COCO\n",
        "\n",
        "Fine-tune on wildlife dataset\n",
        "\n",
        "Augment data to simulate occlusion, lighting variations\n",
        "\n",
        "Use techniques like mosaic augmentation, random cropping\n",
        "\n",
        "5. Addressing Challenges\n",
        "Occlusion: Train with partially occluded animals, use segmentation models to detect partial shapes\n",
        "\n",
        "Nighttime Detection: Augment training with infrared/low-light images; consider adding thermal images if available\n",
        "\n",
        "Incorporate image enhancement pipelines before detection if needed\n",
        "\n",
        "6. Model Evaluation\n",
        "Use mAP, precision-recall, and confusion matrices\n",
        "\n",
        "Test specifically on occluded and nighttime subsets\n",
        "\n",
        "7. Model Deployment\n",
        "Export trained weights\n",
        "\n",
        "Deploy on edge devices or cloud servers connected to cameras\n",
        "\n",
        "Use Detectron2's inference APIs for real-time detection\n",
        "\n",
        "Implement tracking modules (e.g., SORT/Deep SORT) to maintain identities over frames\n",
        "\n",
        "8. Monitoring and Maintenance\n",
        "Continuously collect new data\n",
        "\n",
        "Periodically retrain to improve accuracy and handle new conditions\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.data import detection_utils as utils\n",
        "\n",
        "def custom_mapper(dataset_dict):\n",
        "    dataset_dict = copy.deepcopy(dataset_dict)\n",
        "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
        "\n",
        "    aug = T.AugmentationList([\n",
        "        T.RandomCrop(\"relative\", (0.8, 0.8)),  # crops to simulate occlusion\n",
        "        T.RandomBrightness(0.8, 1.2),            # lighting variations\n",
        "        T.RandomFlip(horizontal=True),\n",
        "    ])\n",
        "    image, transforms = T.apply_augmentations(aug, image)\n",
        "    # apply same transforms to annotations\n",
        "    annos = [utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
        "             for obj in dataset_dict.pop(\"annotations\")]\n",
        "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
        "    dataset_dict[\"instances\"] = utils.annotations_to_instances(annos, image.shape[:2])\n",
        "    return dataset_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "0oZdwHq8mVlr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}