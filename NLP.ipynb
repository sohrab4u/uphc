{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOElWTKAG3HRMzYlumB0j0Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohrab4u/uphc/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is Computational Linguistics and how does it relate to NLP?\n",
        "- Computational Linguistics is a field that combines linguistics and computer science to understand, model, and process human language using computational methods. It seeks to enable computers to interpret, generate, and interact with natural language effectively. Natural Language Processing (NLP) is a subfield of computational linguistics focused on creating practical applications that allow computers to understand and respond to human language, such as chatbots and translation software. In short, computational linguistics provides the theoretical foundation, while NLP applies it to real-world language tasks"
      ],
      "metadata": {
        "id": "9_UQu5JZh49_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Briefly describe the historical evolution of Natural Language Processing.\n",
        "- Natural Language Processing (NLP) has evolved significantly since the 1950s. It began with rule-based systems in the 1950s and 1960s, exemplified by early efforts like the Georgetown-IBM experiment for machine translation and the ELIZA chatbot. In the 1980s and 1990s, statistical methods and machine learning became dominant, improving language understanding with models like Hidden Markov Models. The 2000s and 2010s saw the rise of deep learning and neural networks, with breakthroughs such as word embeddings (Word2Vec) and transformer models like BERT and GPT. Today, NLP continues advancing rapidly with large-scale language models enabling sophisticated text generation and comprehension"
      ],
      "metadata": {
        "id": "m1EvKMSjiauD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#. 3 List and explain three major use cases of NLP in today’s tech industry.\n",
        "  - Three major use cases of Natural Language Processing (NLP) in today’s tech industry are:\n",
        "\n",
        "Customer Service & Virtual Agents: NLP powers AI chatbots and virtual assistants that enable near-human dialogue, responding to customer queries via voice, text, or email, improving response accuracy and personalized engagement.\n",
        "\n",
        "Real-Time Translation & Subtitling: NLP enables real-time multilingual translation and auto-captioning, facilitating communication in international meetings, live streams, and cross-border e-commerce.\n",
        "\n",
        "Sentiment Analysis & Market Intelligence: NLP analyzes large volumes of text data from social media, reviews, financial reports, and customer feedback to ascertain sentiment, trends, and actionable insights for businesses."
      ],
      "metadata": {
        "id": "N7rikxWrioYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What is text normalization and why is it essential in text processing tasks?\n",
        "  - Text normalization is the process of converting text into a consistent, standard format by removing variations such as capitalization differences, punctuation, contractions, and inflections. This makes the text easier to process in NLP tasks by reducing complexity and variability.\n",
        "\n",
        "It is essential because raw text contains many unpredictable variations that can confuse algorithms. Normalization reduces noise in the data, lowers the dimensionality of input, and improves the efficiency and accuracy of models in tasks like text classification, search, and machine learning. Key techniques include lowercasing, stemming, lemmatization, and removing stop words."
      ],
      "metadata": {
        "id": "V73jkdoni3Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Compare and contrast stemming and lemmatization with suitable examples.\n",
        "  - Stemming and lemmatization are both text normalization techniques used in Natural Language Processing (NLP) to reduce words to their base forms, but they differ in approach and accuracy.\n",
        "\n",
        "Stemming: It is a rule-based process that chops off word endings (suffixes) to get the root form (stem), which may not be a valid word. It is faster and simpler but can be less accurate, sometimes producing stems that are not meaningful (e.g., \"running\" → \"runn\"). It does not consider the context or part of speech.\n",
        "\n",
        "Lemmatization: It is a more complex, context-aware process that reduces words to their dictionary base form (lemma) by considering the part of speech and word meaning. It usually requires a dictionary and linguistic knowledge, producing valid words (e.g., \"running\" → \"run\"). It is slower but yields more accurate and meaningful results.\n",
        "\n",
        "Example:\n",
        "\n",
        "Stemming: \"better\" → \"bett\"\n",
        "\n",
        "Lemmatization: \"better\" → \"good\" (correct base form based on meaning)"
      ],
      "metadata": {
        "id": "qbLIvMpFi_5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text:\n",
        "\n",
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org\n",
        "and jenny via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "# Regex pattern to match email addresses\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Extract email addresses\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "print(\"Extracted email addresses:\")\n",
        "for email in emails:\n",
        "    print(email)"
      ],
      "metadata": {
        "id": "jOKdjURykWA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Sample paragraph\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence.\n",
        "It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis,\n",
        "and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Display tokens and their frequency\n",
        "print(\"Tokens and frequency distribution:\")\n",
        "for word, frequency in freq_dist.items():\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "id": "dDEwBr8Rkdjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define custom component to label proper nouns\n",
        "def proper_noun_annotator(doc):\n",
        "    # Identify tokens with POS tag 'PROPN' (proper noun)\n",
        "    proper_nouns = [token for token in doc if token.pos_ == 'PROPN']\n",
        "\n",
        "    # Create spans for proper nouns and label them as 'PROPN_ENTITY'\n",
        "    spans = [Span(doc, token.i, token.i+1, label=\"PROPN_ENTITY\") for token in proper_nouns]\n",
        "\n",
        "    # Add the spans to doc.ents\n",
        "    doc.ents = list(doc.ents) + spans\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "nlp.add_pipe(proper_noun_annotator, after='ner')\n",
        "\n",
        "# Sample text\n",
        "text = \"John and Mary went to London to attend a conference. Meanwhile, Mr. Smith stayed in New York.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the text spans labeled by the custom annotator\n",
        "print(\"Proper Nouns identified by custom annotator:\")\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == \"PROPN_ENTITY\":\n",
        "        print(ent.text)"
      ],
      "metadata": {
        "id": "orDfwAP0kj0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Dataset of example sentences\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Tokenize and preprocess dataset\n",
        "tokenized_data = [simple_preprocess(sentence) for sentence in dataset]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=50, window=3, min_count=1, workers=2, sg=1)\n",
        "\n",
        "# Print some example word vectors and similarity\n",
        "print(\"Vector for 'word':\")\n",
        "print(model.wv['word'])\n",
        "\n",
        "print(\"\\nMost similar words to 'language':\")\n",
        "print(model.wv.most_similar('language', topn=3))"
      ],
      "metadata": {
        "id": "zd25-ZmNkvN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer reviews\n",
        "\n",
        "# Outline with example code snippets illustrating key steps in analyzing customer feedback using NLP\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Step 1: Load customer feedback data (simulate with sample data)\n",
        "customer_reviews = [\n",
        "    \"I love the service! Quick and easy transactions.\",\n",
        "    \"The app crashes frequently. Very frustrating experience.\",\n",
        "    \"Customer support was helpful and resolved my issue quickly.\",\n",
        "    \"Too many hidden fees, not transparent at all.\",\n",
        "    \"Excellent user interface and seamless navigation!\"\n",
        "]\n",
        "\n",
        "# Step 2: Text Cleaning - lowercase, remove special characters, numbers, and extra spaces\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "cleaned_reviews = [clean_text(review) for review in customer_reviews]\n",
        "\n",
        "# Step 3: Tokenization and Stop Words Removal\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [t for t in tokens if t not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "tokenized_reviews = [tokenize_and_remove_stopwords(review) for review in cleaned_reviews]\n",
        "\n",
        "# Step 4: Frequency Analysis - Get most common words\n",
        "all_tokens = [token for sublist in tokenized_reviews for token in sublist]\n",
        "word_freq = Counter(all_tokens)\n",
        "print(\"Most common words:\", word_freq.most_common(5))\n",
        "\n",
        "# Step 5: Sentiment Analysis using TextBlob\n",
        "sentiments = [TextBlob(review).sentiment.polarity for review in cleaned_reviews]\n",
        "print(\"Sentiment scores:\", sentiments)\n",
        "\n",
        "# Step 6: Visualize with Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_tokens))\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5jP0AXW2lDy1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}